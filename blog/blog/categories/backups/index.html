<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: backups | /usr/sbin/blog]]></title>
  <link href="http://usrsb.in/blog/" rel="self"/>
  <link href="http://usrsb.in/blog/"/>
  <updated>2012-08-02T19:32:39-06:00</updated>
  <id>http://usrsb.in/blog/</id>
  <author>
    <name><![CDATA[Alex Beal]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
    <entry>
      




<title type="html"><![CDATA[Secure Versioned Remote Backups with Rdiff-Backup]]></title>
<link href="http://usrsb.in/blog/blog/2011/08/13/secure-versioned-remote-backups-with-rdiff-backup/"/>
<updated>2011-08-13T14:31:00-06:00</updated>
<id>http://usrsb.in/blog/blog/2011/08/13/secure-versioned-remote-backups-with-rdiff-backup</id>

      <content type="html"><![CDATA[<p>When putting together a backup solution for this web server, I was looking for a few things:</p>

<ol>
<li><strong>Simplicity:</strong> The less new software, the better. Rsync is both powerful and complex, and I wanted to avoid it. Other things being equal, simple solutions are also more reliable.</li>
<li><strong>Security:</strong> I needed to prevent an attacker who gained access to my web server from also gaining access to my backup server. Backing up over an encrypted connection was also a necessity.</li>
<li><strong>Versioning:</strong> I needed my solution to keep several versions of all my files. A solution that only keeps the most recent version of a file is useless, if that version turns out to be corrupted.</li>
<li><strong>Incrementing:</strong> An efficient backup solution should only update the files that have changed. This ruled out simply tar-ing and ssh-ing my entire webroot over to my backup server.</li>
<li><strong>Automation:</strong> This solution should automatically run as a cron job.</li>
</ol>


<p>As you can see, some of these goals conflict. Wanting versioned incremental backups, while also ruling out rsync just made my life harder. Automation and security also butt heads. If my web server can access my backup server without a password, how can I prevent an attacker who's taken over my web server from trashing my backups? As I'll show, I managed to piece this together with only one new software package: rdiff-backup. This took care of versioning and incrementing in a simpler way than rsync would have. The rest was achieved with tools installed on every web server: ssh and cron.</p>

<h2>Backing up with rdiff-backup</h2>

<p>Let's start with rdiff-backup. This is dead simple. On Ubuntu, installing is only an apt-get away. This needs to be done on the web server and backup server:</p>

<pre><code>sudo apt-get install rdiff-backup
</code></pre>

<p>Backing up to a remote directory over ssh isn't much harder:</p>

<pre><code>rdiff-backup /var/www backups@example.com::/var/backups
</code></pre>

<p>That will backup the local directory <code>/var/www</code> to <code>/var/backups</code> on <code>example.com</code> using the ssh user <code>backups</code>. It's that simple. rdiff-backup handles the versioning and incrementing automatically. Also, since it's done over ssh, everything is encrypted.
Restoring is just as easy:</p>

<pre><code>rdiff-backup -r 3D backups@example.com::/var/backups/www /var/www.old
</code></pre>

<p>That will copy whatever you backed up to <code>/var/backups/www</code> three days prior to <code>/var/www.old</code> on the local machine. Hopefully by now you see why I love this utility. It's basically rsync, but stripped down and rebuilt specifically for backups. Best of all, there are no config files or servers to maintain.</p>

<h2>Automation with cron</h2>

<p>So, those are the basics of rdiff-backup. How do we automate this? Here's the backup script that I use on this web server:</p>

<pre><code>#!/bin/bash

RDIFF="/usr/bin/rdiff-backup"
REMOTE="backups@example.com::/home/backups/newest/"
CONF="/etc/backups"

cat $CONF | while read DIR; do
        $RDIFF $DIR $REMOTE$(basename $DIR)
        if [ $? != 0 ]; then
                echo "Exited with errors. Backup process stopped at $DIR"
                exit 1;
        fi
done
</code></pre>

<p>What this does is read the paths I've listed in the text file <code>/etc/backups</code>, and backs those up to <code>/home/backups/newest</code> on the machine <code>example.com</code> with the user <code>backups</code>. So if <code>/etc/backups</code> looks like this:</p>

<pre><code>/etc/
/home/alex/
/var/www/
</code></pre>

<p>Then all those directories get backed up to the remote machine. The first time it's run, it simply mirrors the directories to the remote machine. Each consecutive execution only copies the changed files over. Versioning is, of course, handled automatically by rdiff-backup.</p>

<p>I wrote the script to be easily customizable. Change <code>$REMOTE</code> to modify which machine, user, and directory the files are backed up to. Change <code>$CONF</code> to modify which file contains all the paths to be backed up. <code>$RDIFF</code> simply points to wherever the rdiff-backup binary is (well, python script, actually). As it's currently set, <code>$RDIFF</code> should work on Ubuntu 10.04 machines.</p>

<p>To automate this, simply set it as a cron job on the web server, and run it hourly or daily. Make sure the user running the job has high enough privileges to access the files you want to back up (unless you want to set up some complex permissions, simply running this as root is an unfortunate necessity).</p>

<p>Finally, on the remote machine, you may want some sort of script that manages the backup directory. One idea would be to have the backup directory tar-ed and compressed, then saved somewhere safe. Once that's done, the directory could be cleared out, so the next time the backup script runs, a fresh copy of all the files would be copied over. Set this to run once a week and you'll have weekly full backups, with nightly (or hourly) incremental backups. The following script will do that for you:</p>

<pre><code>#!/bin/bash

NEW="/home/backups/newest"
OLD="/home/backups/archive"

# Tar and compress the old files
(
    cd `dirname $NEW`
    tar -zcf "$OLD/`date "+%F-%H%M"`.tar.gz" "`basename "$NEW"`"
)

# Clear out the backup directory
if [ $? == 0 ]; then
    rm -rf "$NEW"
    mkdir "$NEW"
else
    echo "Exited with errors. Nothing was deleted, but the files may not have been rotated."
    exit 1
fi
</code></pre>

<p><code>$NEW</code> is the location of your newest backups. <code>$OLD</code> is where you want the tar-ed and compressed copies stored.</p>

<h2>Security with ssh</h2>

<p>We're almost there. Rdiff-backup is installed, and the scripts are automatically backing up and rotating our backup files. The only problem is that every time the script connects to the backup server, it's asked for a password (the remote user's ssh password). We can't be there to type it in, so how do we deal with this? The solution is to create a public/private key pair that the script can log in with. There are lots of places on the web that have detailed instructions on how to do this, but I'll run through it quickly.</p>

<p>First we decide which user is running the backup script on our web server. If it's root, then we log in as root and run <code>ssh-keygen</code>. When prompted for a password, leave it blank. After this is done, we need to copy the public key located under <code>/root/.ssh/id_rsa.pub</code> to the remote machine. If we're logging in as backups on the remote machine, then we copy the public key into the <code>/home/backups/.ssh</code> directory (create the <code>.ssh</code> directory if it doesn't already exist) and rename the file <code>authorized_keys</code>. Now when root connects over ssh to the backup server, he won't be prompted for a password, and neither will the backup script being run as a root cron job. Ahhh the magic of public key cryptography.</p>

<p>The obvious problem here is that if an attacker gets root on the web server, he has access to the backup server. Lucky for us, ssh has a built in way of restricting a remote user to only one command. We do this by prefixing the public key with <code>command="RESTRICTED_COMMAND"</code>. So, for example, we can restrict a remote user to rdiff-backup by modifying the <code>authorized_keys</code> file to look something like this:</p>

<pre><code>command="rdiff-backup --server --restrict-update-only /home/backups/newest/" ssh-rsa AAB3NzaC1 [...]
</code></pre>

<p>That allows a remote user to only execute rdiff-backup in server mode. But notice the second flag <code>--restrict-update-only</code>. That restricts the user to the backup directory, and only allows her to update the backups, and not delete or otherwise trash them. Pretty cool. The worst an attacker could do is fill the backup server's hard drive by pushing a huge amount of data to it, but since rdiff-backup is versioned, no old versions of the files will be lost.</p>

<p>Also, there are additional options you can prefix to the public key to lock down the server even more. Check out the <code>ssh-keygen</code> man pages, and look under the <code>-O</code> flag.</p>

<h2>Conclusion</h2>

<p>So, that's my custom built backup solution. I realize I glazed over a lot of small details, and this isn't really a step by step how-to (e.g., I never explained how to set up a cron job). I leave it as an exercise to the reader to put it all together. To help you, I've included the scripts at the bottom of this post. I also check the comments obsessively, so please don't hesitate to ask if you have any questions.</p>

<p><a href="http://media.usrsb.in/rdiff-backup/backup.zip">Download the scripts.</a></p>

<p><a rel="bookmark" href="http://usrsb.in/blog/blog/2011/08/13/secure-versioned-remote-backups-with-rdiff-backup/">&infin; Permalink</a></p>

]]></content>
    </entry>
  
</feed>

